{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"****In this notebook I try to find the best features for xgboost. Anybody has anykind of advice or suggestion would appreciated****","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# from reg_funcs import *\n\nfrom sklearn.model_selection import cross_val_score, RepeatedStratifiedKFold, train_test_split, StratifiedKFold\nfrom xgboost import XGBRegressor\nfrom copy import deepcopy\nfrom sklearn.metrics import mean_squared_error, r2_score\nfrom sklearn.preprocessing import StandardScaler\nfrom functools import partial\nfrom sklearn.model_selection import KFold\nfrom sklearn.decomposition import PCA\n\nimport eli5\nfrom eli5.sklearn import PermutationImportance\nfrom sklearn.metrics import make_scorer\nfrom sklearn.inspection import permutation_importance\nfrom sklearn.metrics import RocCurveDisplay, auc\n\n\n\n\nfrom path import Path\nimport warnings \nwarnings.filterwarnings('ignore') # supress warnings","metadata":{"execution":{"iopub.status.busy":"2023-03-29T08:43:11.102260Z","iopub.execute_input":"2023-03-29T08:43:11.102593Z","iopub.status.idle":"2023-03-29T08:43:23.169596Z","shell.execute_reply.started":"2023-03-29T08:43:11.102564Z","shell.execute_reply":"2023-03-29T08:43:23.168405Z"},"trusted":true},"execution_count":1,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<style type='text/css'>\n.datatable table.frame { margin-bottom: 0; }\n.datatable table.frame thead { border-bottom: none; }\n.datatable table.frame tr.coltypes td {  color: #FFFFFF;  line-height: 6px;  padding: 0 0.5em;}\n.datatable .bool    { background: #DDDD99; }\n.datatable .object  { background: #565656; }\n.datatable .int     { background: #5D9E5D; }\n.datatable .float   { background: #4040CC; }\n.datatable .str     { background: #CC4040; }\n.datatable .time    { background: #40CC40; }\n.datatable .row_index {  background: var(--jp-border-color3);  border-right: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  font-size: 9px;}\n.datatable .frame tbody td { text-align: left; }\n.datatable .frame tr.coltypes .row_index {  background: var(--jp-border-color0);}\n.datatable th:nth-child(2) { padding-left: 12px; }\n.datatable .hellipsis {  color: var(--jp-cell-editor-border-color);}\n.datatable .vellipsis {  background: var(--jp-layout-color0);  color: var(--jp-cell-editor-border-color);}\n.datatable .na {  color: var(--jp-cell-editor-border-color);  font-size: 80%;}\n.datatable .sp {  opacity: 0.25;}\n.datatable .footer { font-size: 9px; }\n.datatable .frame_dimensions {  background: var(--jp-border-color3);  border-top: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  display: inline-block;  opacity: 0.6;  padding: 1px 10px 1px 5px;}\n</style>\n"},"metadata":{}}]},{"cell_type":"code","source":"# loading datasets\npath = Path(\"/kaggle/input/playground-series-s3e11\")\n\ntrain = pd.read_csv(path / \"train.csv\")\ntest = pd.read_csv(path / \"test.csv\")\nsub = pd.read_csv(path / \"sample_submission.csv\")\n\n\noriginal_train = pd.read_csv(\"/kaggle/input/media-campaign-cost-prediction/train_dataset.csv\")\noriginal_test = pd.read_csv(\"/kaggle/input/media-campaign-cost-prediction/test_dataset.csv\")\noriginal = pd.concat([original_train, original_test])\noriginal = original[train.drop('id', axis=1).columns]\n\noriginal = original[~original['cost'].isnull()] # removing nulls\n# removing duplicates in original\noriginal = original[~original.drop(\"cost\", axis=1).duplicated()]","metadata":{"execution":{"iopub.status.busy":"2023-03-29T08:43:23.171580Z","iopub.execute_input":"2023-03-29T08:43:23.172826Z","iopub.status.idle":"2023-03-29T08:43:24.858623Z","shell.execute_reply.started":"2023-03-29T08:43:23.172782Z","shell.execute_reply":"2023-03-29T08:43:24.857570Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"# converting ordinal categorical feature to integar\nfeats = ['unit_sales(in millions)', 'total_children', 'num_children_at_home', 'avg_cars_at home(approx).1', 'recyclable_package',\n        'low_fat', 'units_per_case', 'store_sqft', 'coffee_bar', 'video_store', \"salad_bar\", \"prepared_food\", \"florist\"]\n\ntrain[feats] = train[feats].astype('int')\ntest[feats] = test[feats].astype('int')\noriginal[feats] = original[feats].astype('int')","metadata":{"execution":{"iopub.status.busy":"2023-03-28T08:10:49.472840Z","iopub.execute_input":"2023-03-28T08:10:49.473223Z","iopub.status.idle":"2023-03-28T08:10:49.678904Z","shell.execute_reply.started":"2023-03-28T08:10:49.473186Z","shell.execute_reply":"2023-03-28T08:10:49.677717Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"****all the function I used****","metadata":{}},{"cell_type":"code","source":"def evaluate_model(model_name, model, _X, _y, gbdt=True, original_data=None, use_original=False, n_splits=5, random_state_list=[0, 5, 10]):\n    len_y = len(_y)\n    len_states = len(random_state_list)\n\n    oof_preds = np.zeros(len_y * len_states).reshape(len_states, len_y)\n    models = []\n    scores_train = []\n    scalers = []\n    for index, random_state in enumerate(random_state_list):\n        print(\"#\"*25)\n        print(\"#\"*15, f\"traininng model {model_name} with seed {random_state}\")\n        print(\"#\"*25)\n        splitter = Splitter(n_splits=n_splits)\n        splits = 0\n        for X_train, X_val, y_train, y_val, train_idx, val_idx in splitter.split_data(_X, _y, random_state):\n\n            \n            if use_original: # we will only use original data for training not testing\n                target = 'cost'\n                X_train = pd.concat([X_train, original_data.drop(target, axis=1)]) \n                y_train = pd.concat([y_train, np.log(original_data[target])]) # only for  this playground series\n                \n#             all preprocessing will happen after this step\n\n            if not gbdt:  \n                scaler = StandardScaler()\n                X_train = scaler.fit_transform(X_train)\n                X_val = scaler.transform(X_val)\n                scalers.append(deepcopy(scaler))\n            \n            _model = model()\n            _model.fit(X_train, y_train)\n            oof_preds[index, val_idx] = _model.predict(X_val).squeeze()\n            models.append(deepcopy(_model))\n\n            score_train = root_mean_squared_log_error(y_train, _model.predict(X_train))\n            scores_train.append(score_train)\n\n            score_valid_split = root_mean_squared_log_error(y_val, _model.predict(X_val).squeeze())\n\n            print(f\"seed {random_state} and split {splits} score {score_valid_split}\")\n            splits += 1\n\n    oof_preds_mean = oof_preds.mean(axis=0)\n\n    return models, oof_preds_mean, np.mean(scores_train), root_mean_squared_log_error(_y, oof_preds_mean), scalers\n   \n    \n    \ndef predict_test(models, X_test, scalers=None, gbdt=True, n_splits=5, n_repeats=3):\n    test_preds = np.zeros(n_splits * n_repeats * len(X_test)).reshape(n_splits * n_repeats, len(X_test))\n    X_test_ = X_test.copy()\n    for index, model in enumerate(models):\n        if not gbdt:\n            X_test_ = scalers[index].transform(X_test_) # normally you would not do ensembling for linear models \n        preds = model.predict(X_test_)                 # if we have a lot of observations we can do that if we want\n        test_preds[index, range(len(preds))] = preds\n        \n    return test_preds.mean(axis=0)\n\n    \n#  plot correaltion        \ndef plot_corr(df, features, target, sort=False, method='pearson', figsize=(13, 8), use_mask=True, mask_type=\"triu\", **kwargs):\n    \"\"\"Plot correlation given dataset and features names\"\"\"\n    plt.figure(figsize=figsize) # sets figure size\n    corr = df[features].corr(method=method) # calculates correlation based on the features and method provied\n    target_values = corr[target]\n    features.remove(target)\n    corr = corr[features]\n    \n    corr = pd.concat([corr, target_values], axis=1)\n    \n    if sort: # if sort is true then sort the correlation matrix\n        corr = corr.sort_values(by=target)\n    if use_mask: # if uses_mask = True\n        if mask_type == 'triu': # sets mask type to lower trigonal\n            mask = np.triu(np.ones(corr.shape)) \n        else:   # sets mask type to upper trigonal\n            mask = np.tril(np.ones(corr.shape))\n        sns.heatmap(corr, annot=True, mask=mask, **kwargs)\n        \n    else: # if uses mask is not true\n         sns.heatmap(corr, annot=True, **kwargs)       \n\n    plt.title('Correlation between features')\n  \n\n\n\ndef plot_importance(models, X_test, title=\"\"):\n#     taken from https://www.kaggle.com/code/shoabahamed/ps3e9-eda-and-gbdt-catboost-median-duplicatedata/edit\n    \"\"\"Plots features importance given models and train set\"\"\"\n    features = X_test.columns.tolist()\n    feature_importance = pd.DataFrame()\n    for model in models:\n        _df = pd.DataFrame()\n        _df['importance'] = model.feature_importances_\n        _df[\"features\"] = pd.Series(features)\n        _df = _df.sort_values(by='importance', ascending=False)\n        feature_importance = pd.concat([feature_importance, _df])\n        \n                \n    feature_importance = feature_importance.sort_values('importance', ascending=False)\n    plt.figure(figsize=(16, 10))\n    ax = sns.barplot(x='importance', y='features', data=feature_importance, color='skyblue', errorbar='sd')\n    \n    for i in ax.containers:\n        ax.bar_label(i,)\n    \n   \n    plt.xlabel('Importance', fontsize=14)\n    plt.ylabel('Feature', fontsize=14)\n    plt.title(f\"{title} Feature Importances\", fontsize=18)\n    plt.grid(True, axis='x')\n    plt.show()\n    \n    return feature_importance\n\n\n\ndef root_mean_squared_log_error(y_true_log, y_pred_log):\n    return mean_squared_error(y_true_log, y_pred_log, squared=False)\n\n\ndef plot_residuals(df, y_true, y_preds, features, target, **kwargs):\n    \"\"\"Takes the trained dataset, y_true and y_preds. You also have to provide the feature names used in training and target feature\\\n    name\"\"\"\n    cols = 3\n    rows = int(np.ceil(len(features) / 3))\n    \n    fig, ax = plt.subplots(rows, cols, figsize=(15, rows*4))\n    axes =  ax.flatten()\n    \n    if not isinstance(y_true, np.ndarray):\n        y_true = y_true.values\n    if not isinstance(y_preds, np.ndarray):\n        y_preds = y_preds.values\n        \n    y_res = y_true - y_preds\n    \n    for index, feat in enumerate(features):      \n        ax = sns.scatterplot(data=df, x=feat, y=y_res, ax=axes[index], **kwargs)\n\n        ax.axhline(0, color='red', ls='--')\n        ax.set_ylabel('y residuals')\n        if feat == target:\n            ax.set_xlabel(f\"target - {target} true values\")\n            continue\n        ax.set_xlabel(feat)\n\n    plt.suptitle(f\"Residual plots for features\", size=14)\n    plt.tight_layout()\n\n\n\n\nclass Splitter:\n    \"\"\"A splitter class which splits the X, y using the split_data function with a random state provided. It yeilds \\\n    X_train, X_val, y_train, y_val, train_idx, val_idx in the end.\\\n    code from  https://www.kaggle.com/code/tetsutani/ps3e9-eda-and-gbdt-catboost-median-duplicatedata wit little bit of modification \"\"\"\n\n    def __init__(self, test_size=0.2, kfold=True, n_splits=5):\n        self.test_size = test_size # set test size\n        self.kfold = kfold  # wheter to just split the data in two or use kfold\n        self.n_splits= n_splits # set \n        \n    def split_data(self, X, y, random_state):\n        if self.kfold:\n            kf = KFold(n_splits=self.n_splits, random_state=random_state, shuffle=True)\n            for train_idx, val_idx in kf.split(X, y):\n                X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n                y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n                yield X_train, X_val, y_train, y_val, train_idx, val_idx\n        else:\n            X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=self.test_size, random_state=random_state)\n            yield X_train, X_val, y_train, y_val\n\n            \n# plot adversarial_validations scores for each splits\ndef adversarial_validation(train_df, test_df, classifier, n_splits=5, title='Roc curves', figsize=(6, 6)):\n    \"\"\"This is a function which checks if distribution between two datasets are same given the same columns. Then plots roc curve\\\nfor each split. A score close to 0.5 means very similar(Do not provide the target feature)\"\"\"\n    \n    train_df['target'] = 1  \n    test_df['target'] = 0\n    \n    if len(train_df) > len(test_df):\n        size = len(test_df)\n    else:\n        size = len(train_df)\n    \n    train_df = train_df.sample(size)\n    test_df = test_df.sample(size)\n    \n    df = pd.concat([train_df, test_df]).sample(frac=1)\n    \n    X = df.drop('target', axis=1)\n    y = df['target']\n    \n    cv = StratifiedKFold(n_splits=n_splits)\n\n    tprs = []\n    aucs = []\n    mean_fpr = np.linspace(0, 1, 100)\n\n    fig, ax = plt.subplots(figsize=figsize)\n    for fold, (train_idx, val_idx) in enumerate(cv.split(X, y)):\n        classifier.fit(X.iloc[train_idx], y.iloc[train_idx])\n        viz = RocCurveDisplay.from_estimator(\n            classifier,\n            X.iloc[val_idx],\n            y.iloc[val_idx],\n            name=f\"ROC fold {fold}\",\n            alpha=0.3,\n            lw=1,\n            ax=ax,\n        )\n        interp_tpr = np.interp(mean_fpr, viz.fpr, viz.tpr)\n        interp_tpr[0] = 0.0\n        tprs.append(interp_tpr)\n        aucs.append(viz.roc_auc)\n        \n    ax.plot([0, 1], [0, 1], \"k--\", label=\"chance level (AUC = 0.5)\")\n\n    mean_tpr = np.mean(tprs, axis=0)\n    mean_tpr[-1] = 1.0\n    mean_auc = auc(mean_fpr, mean_tpr)\n    std_auc = np.std(aucs)\n    ax.plot(\n        mean_fpr,\n        mean_tpr,\n        color=\"b\",\n        label=r\"Mean ROC (AUC = %0.2f $\\pm$ %0.2f)\" % (mean_auc, std_auc),\n        lw=2,\n        alpha=0.8,\n    )\n\n    std_tpr = np.std(tprs, axis=0)\n    tprs_upper = np.minimum(mean_tpr + std_tpr, 1)\n    tprs_lower = np.maximum(mean_tpr - std_tpr, 0)\n    ax.fill_between(\n        mean_fpr,\n        tprs_lower,\n        tprs_upper,\n        color=\"grey\",\n        alpha=0.2,\n        label=r\"$\\pm$ 1 std. dev.\",\n    )\n\n    ax.set(\n        xlim=[-0.05, 1.05],\n        ylim=[-0.05, 1.05],\n        xlabel=\"False Positive Rate\",\n        ylabel=\"True Positive Rate\",\n        title=f\"{title}\",\n    )\n    ax.axis(\"square\")\n    ax.legend(loc=\"lower right\")\n    plt.show()\n\n\n# plot pca\ndef plot_pca(df):\n    \"\"\"Takes a dataset and do pricinpal component analysis and plots the results\"\"\"\n    #     The code below is mostly taken from kaggle learn course Principal Component Analysis. Link is below\n    # https://www.kaggle.com/code/ryanholbrook/principal-component-analysis#Example---1985-Automobiles\n    _X = df.copy()\n    features = _X.columns.tolist()\n\n    # Standardize\n    X_scaled = (_X - _X.mean(axis=0)) / _X.std(axis=0)\n    \n    pca = PCA()\n    X_pca = pca.fit_transform(X_scaled)\n\n    # Convert to dataframe\n    component_names = [f\"PC{i+1}\" for i in range(X_pca.shape[1])]\n    X_pca = pd.DataFrame(X_pca, columns=component_names)\n\n    loadings = pd.DataFrame(\n        pca.components_.T,  # transpose the matrix of loadings\n        columns=component_names,  # so the columns are the principal components\n        index=_X.columns,  # and the rows are the original features\n    )\n    \n    display(loadings)\n    plot_variance(pca)\n    \n    return X_pca\n\n\n\n# helper function of plot_pca\ndef plot_variance(pca, width=8, dpi=100):\n    \"\"\"\"plot pca vairance given the pca object provided where the dataset is already passed\"\"\"\n    # Create figure\n    fig, axs = plt.subplots(1, 2)\n    n = pca.n_components_\n    grid = np.arange(1, n + 1)\n    # Explained variance\n    evr = pca.explained_variance_ratio_\n    axs[0].bar(grid, evr)\n    axs[0].set(\n        xlabel=\"Component\", title=\"% Explained Variance\", ylim=(0.0, 1.0)\n    )\n    # Cumulative Variance\n    cv = np.cumsum(evr)\n    axs[1].plot(np.r_[0, grid], np.r_[0, cv], \"o-\")\n    axs[1].set(\n        xlabel=\"Component\", title=\"% Cumulative Variance\", ylim=(0.0, 1.0)\n    )\n    # Set up figure\n    fig.set(figwidth=12, dpi=100)\n    return axs\n    ","metadata":{"execution":{"iopub.status.busy":"2023-03-28T08:10:49.682532Z","iopub.execute_input":"2023-03-28T08:10:49.682979Z","iopub.status.idle":"2023-03-28T08:10:49.734988Z","shell.execute_reply.started":"2023-03-28T08:10:49.682938Z","shell.execute_reply":"2023-03-28T08:10:49.733973Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# can try to see if rounding continous features increases the score\nnp.round(train['store_sales(in millions)']).nunique()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-03-28T08:10:49.736622Z","iopub.execute_input":"2023-03-28T08:10:49.737071Z","iopub.status.idle":"2023-03-28T08:10:49.750484Z","shell.execute_reply.started":"2023-03-28T08:10:49.737035Z","shell.execute_reply":"2023-03-28T08:10:49.748901Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"****Correlation****","metadata":{}},{"cell_type":"code","source":"features = train.drop('id', axis=1).columns.tolist()\nplot_corr(train.drop('id', axis=1), features, target='cost', sort=True, cmap=\"Greens\", fmt='.3f', mask_type='tril')","metadata":{"execution":{"iopub.status.busy":"2023-03-28T08:10:49.752916Z","iopub.execute_input":"2023-03-28T08:10:49.753642Z","iopub.status.idle":"2023-03-28T08:10:51.200452Z","shell.execute_reply.started":"2023-03-28T08:10:49.753603Z","shell.execute_reply":"2023-03-28T08:10:51.199353Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"****Baseline Model****\n\nAny experiment we do has must have better than this model to be accepeted","metadata":{}},{"cell_type":"code","source":"X = train.drop(['id', 'cost', 'salad_bar'], axis=1)\ny = np.log(train['cost'])\n\nfeatures = X.columns.tolist()\nxgb = partial(XGBRegressor, tree_method='gpu_hist', random_state=0)","metadata":{"execution":{"iopub.status.busy":"2023-03-28T08:10:51.201624Z","iopub.execute_input":"2023-03-28T08:10:51.203042Z","iopub.status.idle":"2023-03-28T08:10:51.226066Z","shell.execute_reply.started":"2023-03-28T08:10:51.203005Z","shell.execute_reply":"2023-03-28T08:10:51.224878Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"_models, _oof_preds, mean_train_score, mean_valid_score, _scalers = evaluate_model(\"XGBRegressor_default\", xgb, X, y, gbdt=True, \n                                                                            n_splits=10, random_state_list=[0, 5, 10]) ","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2023-03-28T08:10:51.227923Z","iopub.execute_input":"2023-03-28T08:10:51.228716Z","iopub.status.idle":"2023-03-28T08:11:35.716668Z","shell.execute_reply.started":"2023-03-28T08:10:51.228671Z","shell.execute_reply":"2023-03-28T08:11:35.715689Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f\"Default r2 score: {r2_score(y, _oof_preds)}\")\n\nprint(f\"default validation rmsle score: \", mean_valid_score)","metadata":{"execution":{"iopub.status.busy":"2023-03-28T08:11:35.720491Z","iopub.execute_input":"2023-03-28T08:11:35.721302Z","iopub.status.idle":"2023-03-28T08:11:35.732122Z","shell.execute_reply.started":"2023-03-28T08:11:35.721268Z","shell.execute_reply":"2023-03-28T08:11:35.731117Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"****Feature Importance in baseline****","metadata":{}},{"cell_type":"code","source":"feature_importances = plot_importance(_models, test.drop(['id', 'salad_bar'], axis=1), title='Splits')","metadata":{"execution":{"iopub.status.busy":"2023-03-28T08:11:35.737786Z","iopub.execute_input":"2023-03-28T08:11:35.738095Z","iopub.status.idle":"2023-03-28T08:11:36.299900Z","shell.execute_reply.started":"2023-03-28T08:11:35.738067Z","shell.execute_reply":"2023-03-28T08:11:36.298741Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It seems that order of most important features imporatance are constant. Lets try to train the model on whole dataset and see how the plot looks. Although video stores shows some variability with store_sqft","metadata":{}},{"cell_type":"code","source":"_model1 = XGBRegressor(random_state=0, tree_method='gpu_hist', silent=True)\n_model1.fit(X, y)\n\nfeature_importances_all = plot_importance([_model1], test.drop(['id', 'salad_bar'], axis=1), title='All data')","metadata":{"execution":{"iopub.status.busy":"2023-03-28T08:11:36.301386Z","iopub.execute_input":"2023-03-28T08:11:36.302078Z","iopub.status.idle":"2023-03-28T08:11:37.345790Z","shell.execute_reply.started":"2023-03-28T08:11:36.302036Z","shell.execute_reply":"2023-03-28T08:11:37.344776Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Other than store sqft and video store the importance order are mostly similar to ensemble model","metadata":{}},{"cell_type":"markdown","source":"## Residual","metadata":{}},{"cell_type":"code","source":"# in ensemble model\ny_train_preds = predict_test(_models, X, gbdt=True, n_splits=10, n_repeats=3)\nprint(f\"The r2 scoer is {r2_score(y, y_train_preds)} for training\")\nprint(f\"The r2 scoer is {r2_score(y, _oof_preds)} for validataion\")","metadata":{"execution":{"iopub.status.busy":"2023-03-28T08:11:37.347544Z","iopub.execute_input":"2023-03-28T08:11:37.348345Z","iopub.status.idle":"2023-03-28T08:11:58.818070Z","shell.execute_reply.started":"2023-03-28T08:11:37.348303Z","shell.execute_reply":"2023-03-28T08:11:58.815871Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.distplot(y)\nsns.distplot(_oof_preds)\nplt.legend(['train true values', 'train_predictions'])\nplt.title(\"Distribution of true and predicted\");","metadata":{"execution":{"iopub.status.busy":"2023-03-28T08:11:58.819709Z","iopub.execute_input":"2023-03-28T08:11:58.820105Z","iopub.status.idle":"2023-03-28T08:12:01.997143Z","shell.execute_reply.started":"2023-03-28T08:11:58.820066Z","shell.execute_reply":"2023-03-28T08:12:01.995929Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Our predictions dispite scoring well in the leaderboard they actually does not follow the same distribution as training . Lets dig a little depper and see if we can find out how much our model was not able to learn from residuals","metadata":{}},{"cell_type":"code","source":"plot_residuals(train, y, _oof_preds, features+['cost'], 'cost')","metadata":{"execution":{"iopub.status.busy":"2023-03-28T08:12:01.998849Z","iopub.execute_input":"2023-03-28T08:12:01.999208Z","iopub.status.idle":"2023-03-28T08:12:24.215570Z","shell.execute_reply.started":"2023-03-28T08:12:01.999173Z","shell.execute_reply":"2023-03-28T08:12:24.208878Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Permutaition Importance\n\nYou can learn about Permutation Importance from [here](https://www.kaggle.com/code/dansbecker/permutation-importance). I also liked how Permutation Importance was displayed in this notebook by https://www.kaggle.com/code/janmpia/what-you-need-to-know-about-this-competition by <b>J€ANMPIA</b>","metadata":{}},{"cell_type":"code","source":"scorer = make_scorer(root_mean_squared_log_error, greater_is_better=False)","metadata":{"execution":{"iopub.status.busy":"2023-03-28T08:12:24.217417Z","iopub.execute_input":"2023-03-28T08:12:24.218055Z","iopub.status.idle":"2023-03-28T08:12:24.223033Z","shell.execute_reply.started":"2023-03-28T08:12:24.218016Z","shell.execute_reply":"2023-03-28T08:12:24.221557Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# for different splitting\nsplitter = Splitter(kfold=False)\nX_train, X_val, y_train, y_val = next(splitter.split_data(X, y, random_state=0))\nfeats = X_train.columns.tolist()\n\n_model = partial(XGBRegressor, random_state=0, tree_method='hist')()\n_model.fit(X_train, y_train)\n\nperm = PermutationImportance(_model, scoring=scorer).fit(X_val, y_val)\neli5.show_weights(perm, feature_names=feats)","metadata":{"execution":{"iopub.status.busy":"2023-03-28T08:12:24.224596Z","iopub.execute_input":"2023-03-28T08:12:24.225236Z","iopub.status.idle":"2023-03-28T08:12:36.462765Z","shell.execute_reply.started":"2023-03-28T08:12:24.225201Z","shell.execute_reply":"2023-03-28T08:12:36.461651Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"splitter = Splitter(kfold=False)\nX_train, X_val, y_train, y_val = next(splitter.split_data(X, y, random_state=10)) \n                                                                                 \n_model = partial(XGBRegressor, random_state=0, tree_method='hist')()\n_model.fit(X_train, y_train)\n\nperm = PermutationImportance(_model, random_state=0, scoring=scorer).fit(X_val, y_val)\neli5.show_weights(perm, feature_names=X.columns.tolist())","metadata":{"execution":{"iopub.status.busy":"2023-03-28T08:12:36.464229Z","iopub.execute_input":"2023-03-28T08:12:36.464885Z","iopub.status.idle":"2023-03-28T08:12:49.444452Z","shell.execute_reply.started":"2023-03-28T08:12:36.464846Z","shell.execute_reply":"2023-03-28T08:12:49.443560Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Here we can see that store sqft is the most important feature which is incosistent with feature importance plot above. Although florist is the second most important one. Another thing to notice is that low_fat and recyclable_package shows very little negative values which mean that it was probably not an important feature or it just happend by chance. Anyway we can try removing these features than train our model to submit.(I have tried that and both leaderboard and local valid cv score has increased very little","metadata":{}},{"cell_type":"markdown","source":"Problems we need to deal with:\n* First of all model is not learning enough pattern from the features. It could be because of two much noise or too few features. It can also happen because of bad hyperparamters.\n     \nHow to solve it:\n* Try to add more data and see if works(adding original)\n* Then we will do PCA and see if it reduces our noise in the features\n* Second we will try to come up with new features from existing ones or remove features during training: 'gross_weight', 'low_fat', 'recyclable_package', 'units_per_case' were unimportant in correlation, feature_importance, permutation importance. So we will remove them","metadata":{}},{"cell_type":"markdown","source":"## Adding original","metadata":{}},{"cell_type":"code","source":"train_all = pd.concat([train.drop('id', axis=1), original])\ntrain_all.head(2)","metadata":{"execution":{"iopub.status.busy":"2023-03-28T08:12:49.447352Z","iopub.execute_input":"2023-03-28T08:12:49.448007Z","iopub.status.idle":"2023-03-28T08:12:49.515857Z","shell.execute_reply.started":"2023-03-28T08:12:49.447966Z","shell.execute_reply":"2023-03-28T08:12:49.514734Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_all.drop(['cost'], axis=1).duplicated(keep=False).sum()","metadata":{"execution":{"iopub.status.busy":"2023-03-28T08:12:49.517264Z","iopub.execute_input":"2023-03-28T08:12:49.518052Z","iopub.status.idle":"2023-03-28T08:12:49.695060Z","shell.execute_reply.started":"2023-03-28T08:12:49.518004Z","shell.execute_reply":"2023-03-28T08:12:49.694017Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"for the same observation 13864 instances where cost is different which 3% of the whole dataset. we will only keep the first ones\nwhich reperesent the train dataset ","metadata":{}},{"cell_type":"code","source":"train_all = train_all.loc[~train_all.drop(['cost'], axis=1).duplicated(keep='first')] # only taking the first occurences","metadata":{"execution":{"iopub.status.busy":"2023-03-28T08:12:49.696523Z","iopub.execute_input":"2023-03-28T08:12:49.697013Z","iopub.status.idle":"2023-03-28T08:12:49.858402Z","shell.execute_reply.started":"2023-03-28T08:12:49.696975Z","shell.execute_reply":"2023-03-28T08:12:49.857175Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"lets see if there are any matches between train all and test","metadata":{}},{"cell_type":"code","source":"pd.concat([train_all.drop(columns=['cost']), test.drop('id', axis=1)]).duplicated(keep=False).sum()","metadata":{"execution":{"iopub.status.busy":"2023-03-28T08:12:49.860155Z","iopub.execute_input":"2023-03-28T08:12:49.860771Z","iopub.status.idle":"2023-03-28T08:12:50.069640Z","shell.execute_reply.started":"2023-03-28T08:12:49.860729Z","shell.execute_reply":"2023-03-28T08:12:50.068529Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There are few matches between train_all and test due to including original. Lets check if distribution is similar in train cost and original cost","metadata":{}},{"cell_type":"code","source":"from lightgbm import LGBMClassifier","metadata":{"execution":{"iopub.status.busy":"2023-03-28T08:12:50.071454Z","iopub.execute_input":"2023-03-28T08:12:50.071887Z","iopub.status.idle":"2023-03-28T08:12:50.077490Z","shell.execute_reply.started":"2023-03-28T08:12:50.071844Z","shell.execute_reply":"2023-03-28T08:12:50.076209Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"adversarial_validation(original[['cost']], train[['cost']], LGBMClassifier())","metadata":{"execution":{"iopub.status.busy":"2023-03-28T08:12:50.079382Z","iopub.execute_input":"2023-03-28T08:12:50.079845Z","iopub.status.idle":"2023-03-28T08:12:53.130806Z","shell.execute_reply.started":"2023-03-28T08:12:50.079784Z","shell.execute_reply":"2023-03-28T08:12:53.129757Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"distribution of orginal cost and train cost is the same. We can see there are not much difference in distribtuion between train data original data","metadata":{}},{"cell_type":"code","source":"adversarial_validation(original.drop('cost', axis=1), train.drop(['id', 'cost'], axis=1), LGBMClassifier())","metadata":{"execution":{"iopub.status.busy":"2023-03-28T08:12:53.132244Z","iopub.execute_input":"2023-03-28T08:12:53.135234Z","iopub.status.idle":"2023-03-28T08:12:58.458127Z","shell.execute_reply.started":"2023-03-28T08:12:53.135203Z","shell.execute_reply":"2023-03-28T08:12:58.457039Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Okay lets train the model and see how much our score increases or decreases by including original data","metadata":{}},{"cell_type":"code","source":"org = original.copy()\ntrain_temp = train.copy()\n\norg['generated'] = False\ntrain_temp['generated'] = True\n\ndata = pd.concat([train_temp.drop('id', axis=1), org])\ndata = data[~data.drop(['cost', 'generated'], axis=1).duplicated(keep='first')]\n\ndata.head()","metadata":{"execution":{"iopub.status.busy":"2023-03-28T08:12:58.460572Z","iopub.execute_input":"2023-03-28T08:12:58.461593Z","iopub.status.idle":"2023-03-28T08:12:58.787511Z","shell.execute_reply.started":"2023-03-28T08:12:58.461552Z","shell.execute_reply":"2023-03-28T08:12:58.786359Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_all = train.drop(['id', 'cost', 'salad_bar'], axis=1)\ny_all = np.log(train['cost'])\nfeatures_all = X_all.columns.tolist()\nadd_data = data.loc[data['generated'] == False]\nadd_data = add_data[features_all + ['cost']]","metadata":{"execution":{"iopub.status.busy":"2023-03-28T08:12:58.789450Z","iopub.execute_input":"2023-03-28T08:12:58.789847Z","iopub.status.idle":"2023-03-28T08:12:58.818174Z","shell.execute_reply.started":"2023-03-28T08:12:58.789808Z","shell.execute_reply":"2023-03-28T08:12:58.817170Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"_models_all, _oof_preds_all, mean_train_score_all, mean_valid_score_all, _scalers_all = evaluate_model(\"XGBRegressor\", xgb, X_all, y_all, \n                                                                            gbdt=True, use_original=True, original_data=add_data,\n                                                                            n_splits=10, random_state_list=[0, 5, 10]) ","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2023-03-28T08:12:58.821294Z","iopub.execute_input":"2023-03-28T08:12:58.821608Z","iopub.status.idle":"2023-03-28T08:13:44.311210Z","shell.execute_reply.started":"2023-03-28T08:12:58.821581Z","shell.execute_reply":"2023-03-28T08:13:44.310163Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f\"valid score default: {mean_valid_score}\")\nprint(f\"valid score with all data addded: {mean_valid_score_all}\")","metadata":{"execution":{"iopub.status.busy":"2023-03-28T08:13:44.321205Z","iopub.execute_input":"2023-03-28T08:13:44.322185Z","iopub.status.idle":"2023-03-28T08:13:44.328056Z","shell.execute_reply.started":"2023-03-28T08:13:44.322144Z","shell.execute_reply":"2023-03-28T08:13:44.326932Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_preds_all = predict_test(_models_all, test[features_all], scalers=_scalers_all, gbdt=True, n_splits=10, n_repeats=3)","metadata":{"execution":{"iopub.status.busy":"2023-03-28T08:13:44.329708Z","iopub.execute_input":"2023-03-28T08:13:44.330518Z","iopub.status.idle":"2023-03-28T08:13:58.514788Z","shell.execute_reply.started":"2023-03-28T08:13:44.330480Z","shell.execute_reply":"2023-03-28T08:13:58.513765Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def submission_csv(predictions, target='cost'):\n    df = pd.DataFrame()\n    df['id'] = test['id']\n    df[target] = np.exp(predictions) # exp only for this rmsle\n    \n    return df\n\ndf_all_data = submission_csv(test_preds_all, target='cost')","metadata":{"execution":{"iopub.status.busy":"2023-03-28T08:13:58.516083Z","iopub.execute_input":"2023-03-28T08:13:58.516471Z","iopub.status.idle":"2023-03-28T08:13:58.542748Z","shell.execute_reply.started":"2023-03-28T08:13:58.516430Z","shell.execute_reply":"2023-03-28T08:13:58.541651Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# df_all_data.to_csv(\"xgb_all_data_correct.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2023-03-28T08:13:58.544411Z","iopub.execute_input":"2023-03-28T08:13:58.544864Z","iopub.status.idle":"2023-03-28T08:13:58.550796Z","shell.execute_reply.started":"2023-03-28T08:13:58.544828Z","shell.execute_reply":"2023-03-28T08:13:58.549824Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The score has very little. But I am not sure even though the score if it is worth it to include original. check this thread https://www.kaggle.com/competitions/playground-series-s3e11/discussion/397431 by J€ANMPIA.\n\nBut we will include original as it has increased our score in both local cv and leaderboard although not much","metadata":{}},{"cell_type":"markdown","source":"## Removing Less important features\n\nRemoving noisy or unimportant features may improve our r2score and rmsle. ","metadata":{}},{"cell_type":"code","source":"X_all_rm = train.drop(columns=['id', 'cost', 'salad_bar', 'gross_weight', 'low_fat', 'recyclable_package', 'units_per_case'])\ny_all_rm = np.log(train['cost'])\nfeatures_all_rm = X_all_rm.columns.tolist()\n\n_models_all_rm, _oof_preds_all_rm, mean_train_score_all_rm, mean_valid_score_all_rm, _scalers_all_rm = evaluate_model(\"XGBRegressor\", \n                                    xgb, X_all_rm, y_all_rm, gbdt=True, n_splits=10, random_state_list=[0, 5, 10]) ","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2023-03-28T08:13:58.552410Z","iopub.execute_input":"2023-03-28T08:13:58.552775Z","iopub.status.idle":"2023-03-28T08:14:36.008572Z","shell.execute_reply.started":"2023-03-28T08:13:58.552739Z","shell.execute_reply":"2023-03-28T08:14:36.007699Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f\"Default r2 score: {r2_score(y, _oof_preds)}\")\nprint(f\"r2 score after removing 'gross_weight', 'low_fat', 'recyclable_package', 'units_per_case': {r2_score(y_all_rm, _oof_preds_all_rm)}\")\n\nprint()\n\nprint(f\"default validation rmsle score: \", mean_valid_score)\nprint(f\"validation rmslse after removing 'gross_weight', 'low_fat', 'recyclable_package', 'units_per_case': \",mean_valid_score_all_rm)","metadata":{"execution":{"iopub.status.busy":"2023-03-28T08:14:36.010261Z","iopub.execute_input":"2023-03-28T08:14:36.011069Z","iopub.status.idle":"2023-03-28T08:14:36.024758Z","shell.execute_reply.started":"2023-03-28T08:14:36.011027Z","shell.execute_reply":"2023-03-28T08:14:36.023627Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_preds_all_rm = predict_test(_models_all_rm, test[features_all_rm], gbdt=True, n_splits=10, n_repeats=3)","metadata":{"execution":{"iopub.status.busy":"2023-03-28T08:14:36.026775Z","iopub.execute_input":"2023-03-28T08:14:36.027158Z","iopub.status.idle":"2023-03-28T08:14:49.894542Z","shell.execute_reply.started":"2023-03-28T08:14:36.027122Z","shell.execute_reply":"2023-03-28T08:14:49.893509Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"xgb_model_all_rm = submission_csv(test_preds_all_rm)\nxgb_model_all_rm.head()","metadata":{"execution":{"iopub.status.busy":"2023-03-28T08:14:49.896065Z","iopub.execute_input":"2023-03-28T08:14:49.896623Z","iopub.status.idle":"2023-03-28T08:14:49.926293Z","shell.execute_reply.started":"2023-03-28T08:14:49.896586Z","shell.execute_reply":"2023-03-28T08:14:49.925208Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# xgb_model_all_rm.to_csv(\"xgb_model_all_rm_correct.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2023-03-28T08:14:49.927655Z","iopub.execute_input":"2023-03-28T08:14:49.928132Z","iopub.status.idle":"2023-03-28T08:14:49.933095Z","shell.execute_reply.started":"2023-03-28T08:14:49.928092Z","shell.execute_reply":"2023-03-28T08:14:49.931985Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## PCA","metadata":{}},{"cell_type":"code","source":"X_pca = plot_pca(X)","metadata":{"execution":{"iopub.status.busy":"2023-03-28T08:14:49.935289Z","iopub.execute_input":"2023-03-28T08:14:49.935682Z","iopub.status.idle":"2023-03-28T08:14:50.701693Z","shell.execute_reply.started":"2023-03-28T08:14:49.935645Z","shell.execute_reply":"2023-03-28T08:14:50.700737Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_pca.head()","metadata":{"execution":{"iopub.status.busy":"2023-03-28T08:14:50.703115Z","iopub.execute_input":"2023-03-28T08:14:50.703568Z","iopub.status.idle":"2023-03-28T08:14:50.725783Z","shell.execute_reply.started":"2023-03-28T08:14:50.703530Z","shell.execute_reply":"2023-03-28T08:14:50.724847Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"_models_pca, _oof_preds_pca, mean_train_score_pca, mean_valid_score_pca, _scalers_pca = evaluate_model(\"XGBRegressor_default\", \n                                                                xgb, X_pca, y, gbdt=True, n_splits=10, random_state_list=[0, 5, 10]) ","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2023-03-28T08:14:50.727288Z","iopub.execute_input":"2023-03-28T08:14:50.727648Z","iopub.status.idle":"2023-03-28T08:15:32.640377Z","shell.execute_reply.started":"2023-03-28T08:14:50.727613Z","shell.execute_reply":"2023-03-28T08:15:32.639590Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f\"pca trained local valid score {mean_valid_score_pca}\")\nsns.displot(_oof_preds_pca);","metadata":{"execution":{"iopub.status.busy":"2023-03-28T08:15:32.643707Z","iopub.execute_input":"2023-03-28T08:15:32.645650Z","iopub.status.idle":"2023-03-28T08:15:33.912827Z","shell.execute_reply.started":"2023-03-28T08:15:32.645606Z","shell.execute_reply":"2023-03-28T08:15:33.910810Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Well pca is not helping. Lets try to see if creating new features helps. We will do this using \n- feature importance plot\n- permutation imporatnce\n- partial importance \n- correlation (which will not apply in this dataset)\n- shap\n\nWe have done feature importance, correlation and permutation importance before","metadata":{"execution":{"iopub.status.busy":"2023-03-23T22:34:06.414264Z","iopub.execute_input":"2023-03-23T22:34:06.414717Z","iopub.status.idle":"2023-03-23T22:34:07.741352Z","shell.execute_reply.started":"2023-03-23T22:34:06.414676Z","shell.execute_reply":"2023-03-23T22:34:07.738990Z"}}},{"cell_type":"code","source":"# from pca, feature_importance and permutation computations and after some trial and error\ntrain_temp = train.copy(deep=True)\n\ntrain_temp['extra_attraction'] = train_temp['florist'] + train_temp['video_store'] + train_temp['prepared_food'] + train_temp['coffee_bar']\ntrain_temp['children'] = train_temp['total_children'] * train_temp['num_children_at_home']\n\nplot_corr(train_temp, train_temp.columns.tolist(), target='cost', cmap='Greens', sort=True, mask_type='tril', use_mask=False, fmt='.2f')","metadata":{"execution":{"iopub.status.busy":"2023-03-28T08:15:33.915884Z","iopub.execute_input":"2023-03-28T08:15:33.916167Z","iopub.status.idle":"2023-03-28T08:15:36.600350Z","shell.execute_reply.started":"2023-03-28T08:15:33.916140Z","shell.execute_reply":"2023-03-28T08:15:36.599187Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# display(train_temp.pivot_table(values='id', columns='num_children_at_home', index='total_children', aggfunc='count'))\n# filt = train_temp['num_children_at_home'] > train_temp['total_children']\n# train_temp.loc[filt, 'num_children_at_home'] = train_temp.loc[filt, 'total_children']\n# train_temp.pivot_table(values='id', columns='num_children_at_home', index='total_children', aggfunc='count')","metadata":{"execution":{"iopub.status.busy":"2023-03-28T08:15:36.601724Z","iopub.execute_input":"2023-03-28T08:15:36.602174Z","iopub.status.idle":"2023-03-28T08:15:36.607009Z","shell.execute_reply.started":"2023-03-28T08:15:36.602138Z","shell.execute_reply":"2023-03-28T08:15:36.606061Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# removing more features and adding feature engineered features\nX_all_rms = train.drop(columns=['id', 'cost', 'salad_bar', 'gross_weight', 'low_fat', 'recyclable_package', 'units_per_case', 'store_sales(in millions)', 'unit_sales(in millions)'])\ny_all_rms = np.log(train['cost'])\nfeatures_all_rms = X_all_rms.columns.tolist()\nadd_data = data.loc[data['generated'] == False]\nadd_data = add_data[features_all_rms + ['cost']]\n\n# creating features\nX_all_rms['extra_attraction'] = X_all_rms['florist'] + X_all_rms['video_store'] + X_all_rms['prepared_food'] + X_all_rm['coffee_bar']\nX_all_rms['children'] = X_all_rms['total_children'] * X_all_rms['num_children_at_home']\n\nadd_data['extra_attraction'] = add_data['florist'] + add_data['video_store'] + add_data['prepared_food'] + add_data['coffee_bar']\nadd_data['children'] = add_data['total_children'] * add_data['num_children_at_home']\n\nfeatures_all_rms = X_all_rms.columns.tolist()\n\n_models_all_rms, _oof_preds_all_rms, mean_train_score_all_rms, mean_valid_score_all_rms, _scalers_all_rms = evaluate_model(\"XGBRegressor_default\", xgb, \n                                                                X_all_rms, y_all_rms , use_original=True, original_data=add_data, gbdt=True, n_splits=10, random_state_list=[0, 5, 10]) ","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2023-03-28T08:15:36.608337Z","iopub.execute_input":"2023-03-28T08:15:36.609025Z","iopub.status.idle":"2023-03-28T08:16:16.309784Z","shell.execute_reply.started":"2023-03-28T08:15:36.608986Z","shell.execute_reply":"2023-03-28T08:16:16.308898Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f\"Default r2 score: {r2_score(y, _oof_preds)}\")\nprint(f\"r2 score after removing 'gross_weight', 'low_fat', 'recyclable_package', 'units_per_case': {r2_score(y_all_rms, _oof_preds_all_rms)}\")\n\nprint()\n\nprint(f\"default validation rmsle score: \", mean_valid_score)\nprint(f\"validation rmslse after removing 'gross_weight', 'low_fat', 'recyclable_package', 'units_per_case': \",mean_valid_score_all_rms)","metadata":{"execution":{"iopub.status.busy":"2023-03-28T08:16:16.313281Z","iopub.execute_input":"2023-03-28T08:16:16.315404Z","iopub.status.idle":"2023-03-28T08:16:16.329705Z","shell.execute_reply.started":"2023-03-28T08:16:16.315368Z","shell.execute_reply":"2023-03-28T08:16:16.328573Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_temp = test.copy()\ntest_temp['extra_attraction'] = test_temp['florist'] + test_temp['video_store'] + test_temp['prepared_food'] + test_temp['coffee_bar']\ntest_temp['children'] = test_temp['total_children'] * test_temp['num_children_at_home']\n\ntest_preds_all_rms = predict_test(_models_all_rms, test_temp[features_all_rms], gbdt=True, n_splits=10, n_repeats=3)","metadata":{"execution":{"iopub.status.busy":"2023-03-28T08:16:16.331287Z","iopub.execute_input":"2023-03-28T08:16:16.333510Z","iopub.status.idle":"2023-03-28T08:16:30.136071Z","shell.execute_reply.started":"2023-03-28T08:16:16.333471Z","shell.execute_reply":"2023-03-28T08:16:30.135017Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_all_rms = submission_csv(test_preds_all_rms)\ndf_all_rms.head()","metadata":{"execution":{"iopub.status.busy":"2023-03-28T08:16:30.137543Z","iopub.execute_input":"2023-03-28T08:16:30.137967Z","iopub.status.idle":"2023-03-28T08:16:30.166399Z","shell.execute_reply.started":"2023-03-28T08:16:30.137926Z","shell.execute_reply":"2023-03-28T08:16:30.165227Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# df_all_rms.to_csv(\"xgb_all_rms.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2023-03-28T08:16:30.168990Z","iopub.execute_input":"2023-03-28T08:16:30.169752Z","iopub.status.idle":"2023-03-28T08:16:30.174177Z","shell.execute_reply.started":"2023-03-28T08:16:30.169713Z","shell.execute_reply":"2023-03-28T08:16:30.172972Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"****partial dependence plot****\n\nLearn more about parital plot from [here](https://www.kaggle.com/code/dansbecker/partial-plots/tutorial)","metadata":{}},{"cell_type":"code","source":"X_par = train.drop(columns=['id', 'cost', 'salad_bar', 'gross_weight', 'low_fat', 'recyclable_package', 'units_per_case'])\ny_par = np.log(train['cost'])\nfeatures_par = X_par.columns.tolist()\nadd_data = data.loc[data['generated'] == False]\nadd_data = add_data[features_par+ ['cost']]\n\n\n\n\nsplitter = Splitter(kfold=False)\nX_train, X_val, y_train, y_val = next(splitter.split_data(X_par, y_par, random_state=5))\nX_train = pd.concat([X_train, add_data.drop('cost', axis=1)])\ny_train = pd.concat([y_train, np.log(add_data['cost'])])\n\n_model_par = xgb()\n_model_par.fit(X_train, y_train)","metadata":{"execution":{"iopub.status.busy":"2023-03-28T08:16:30.176055Z","iopub.execute_input":"2023-03-28T08:16:30.176500Z","iopub.status.idle":"2023-03-28T08:16:30.879090Z","shell.execute_reply.started":"2023-03-28T08:16:30.176455Z","shell.execute_reply":"2023-03-28T08:16:30.877700Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.inspection import PartialDependenceDisplay\n\nfeat_name = 'florist'\n\nPartialDependenceDisplay.from_estimator(_model_par, X_val, features=[feat_name])\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-03-28T08:16:30.880735Z","iopub.execute_input":"2023-03-28T08:16:30.881508Z","iopub.status.idle":"2023-03-28T08:16:31.352702Z","shell.execute_reply.started":"2023-03-28T08:16:30.881469Z","shell.execute_reply":"2023-03-28T08:16:31.351809Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.inspection import PartialDependenceDisplay\n\nfeat_name = 'florist'\n\nfig, ax = plt.subplots(1, 1, figsize=(6, 5))\nax = PartialDependenceDisplay.from_estimator(_model_par, X_val, features=[feat_name], kind='both', ax=ax)\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-03-28T08:16:31.354341Z","iopub.execute_input":"2023-03-28T08:16:31.355120Z","iopub.status.idle":"2023-03-28T08:16:33.250224Z","shell.execute_reply.started":"2023-03-28T08:16:31.355080Z","shell.execute_reply":"2023-03-28T08:16:33.249055Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.boxplot(data=train, x='florist', y='cost')","metadata":{"execution":{"iopub.status.busy":"2023-03-28T08:16:33.252194Z","iopub.execute_input":"2023-03-28T08:16:33.252625Z","iopub.status.idle":"2023-03-28T08:16:33.479922Z","shell.execute_reply.started":"2023-03-28T08:16:33.252581Z","shell.execute_reply":"2023-03-28T08:16:33.478739Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"we can that in our model there is  a downward trend in although the trend is different each time ther and there are some times where the trend is upwards","metadata":{}},{"cell_type":"code","source":"feat_name = ('video_store', 'florist')\n\n\nfig, ax = plt.subplots(1, 1, figsize=(6, 6))\nax = PartialDependenceDisplay.from_estimator(_model_par, X_val, features=[feat_name], kind='average', ax=ax)\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-03-28T08:16:33.481242Z","iopub.execute_input":"2023-03-28T08:16:33.481612Z","iopub.status.idle":"2023-03-28T08:16:34.258752Z","shell.execute_reply.started":"2023-03-28T08:16:33.481573Z","shell.execute_reply":"2023-03-28T08:16:34.257732Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"feat_name = ('coffee_bar', 'florist')\n\n\nfig, ax = plt.subplots(1, 1, figsize=(6, 6))\nax = PartialDependenceDisplay.from_estimator(_model_par, X_val, features=[feat_name], kind='average', ax=ax)\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-03-28T08:16:34.260375Z","iopub.execute_input":"2023-03-28T08:16:34.261028Z","iopub.status.idle":"2023-03-28T08:16:35.036947Z","shell.execute_reply.started":"2023-03-28T08:16:34.260988Z","shell.execute_reply":"2023-03-28T08:16:35.035811Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"feat_name = ('prepared_food', 'florist')\n\n\nfig, ax = plt.subplots(1, 1, figsize=(6, 6))\nax = PartialDependenceDisplay.from_estimator(_model_par, X_val, features=[feat_name], kind='average', ax=ax)\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-03-28T08:16:35.038695Z","iopub.execute_input":"2023-03-28T08:16:35.039441Z","iopub.status.idle":"2023-03-28T08:16:36.103740Z","shell.execute_reply.started":"2023-03-28T08:16:35.039394Z","shell.execute_reply":"2023-03-28T08:16:36.102714Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"All of these extra features shows a downward trend when they are together. We have captured that before with adding them together","metadata":{}},{"cell_type":"code","source":"feat_name = ('total_children', 'num_children_at_home')\n\n\nfig, ax = plt.subplots(1, 1, figsize=(6, 6))\nax = PartialDependenceDisplay.from_estimator(_model_par, X_val, features=[feat_name], kind='average', ax=ax)\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-03-28T08:16:36.105431Z","iopub.execute_input":"2023-03-28T08:16:36.106128Z","iopub.status.idle":"2023-03-28T08:16:40.837858Z","shell.execute_reply.started":"2023-03-28T08:16:36.106089Z","shell.execute_reply":"2023-03-28T08:16:40.836792Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"feat_name = ('avg_cars_at home(approx).1', 'total_children')\n\n\nfig, ax = plt.subplots(1, 1, figsize=(6, 6))\nax = PartialDependenceDisplay.from_estimator(_model_par, X_val, features=[feat_name], kind='average', ax=ax)\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-03-28T08:16:40.839778Z","iopub.execute_input":"2023-03-28T08:16:40.840508Z","iopub.status.idle":"2023-03-28T08:16:44.867997Z","shell.execute_reply.started":"2023-03-28T08:16:40.840468Z","shell.execute_reply":"2023-03-28T08:16:44.866774Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"avg_car_at_home shows similar pattern with total children as no_children_at_home. So multiplying all of them together may increase our score","metadata":{}},{"cell_type":"code","source":"X_all_rms = train.drop(columns=['id', 'cost', 'salad_bar', 'gross_weight', 'low_fat', 'recyclable_package', 'units_per_case', 'store_sales(in millions)', 'unit_sales(in millions)'])\ny_all_rms = np.log(train['cost'])\nfeatures_all_rms = X_all_rms.columns.tolist()\nadd_data = data.loc[data['generated'] == False]\nadd_data = add_data[features_all_rms + ['cost']]\n\n# creating features\nX_all_rms['extra_attraction'] = X_all_rms['florist'] + X_all_rms['video_store'] + X_all_rms['prepared_food'] + X_all_rms['coffee_bar']\nX_all_rms['children'] = X_all_rms['total_children'] * X_all_rms['num_children_at_home']\nX_all_rms['children*avg_cars_at home(approx).1'] = X_all_rms['total_children'] * X_all_rms['num_children_at_home'] * X_all_rms['avg_cars_at home(approx).1']\n\nadd_data['extra_attraction'] = add_data['florist'] + add_data['video_store'] + add_data['prepared_food'] + add_data['coffee_bar']\nadd_data['children'] = add_data['total_children'] * add_data['num_children_at_home']\nadd_data['children*avg_cars_at home(approx).1'] = add_data['total_children'] * add_data['num_children_at_home']* add_data['avg_cars_at home(approx).1']\n\nfeatures_all_rms = X_all_rms.columns.tolist()\n\n_models_all_rms, _oof_preds_all_rms, mean_train_score_all_rms, mean_valid_score_all_rms, _scalers_all_rms = evaluate_model(\"XGBRegressor_default\", xgb, \n                                                                X_all_rms, y_all_rms , use_original=True, original_data=add_data, gbdt=True, n_splits=10, random_state_list=[0, 5, 10]) ","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2023-03-28T08:16:44.869430Z","iopub.execute_input":"2023-03-28T08:16:44.869914Z","iopub.status.idle":"2023-03-28T08:17:26.031585Z","shell.execute_reply.started":"2023-03-28T08:16:44.869874Z","shell.execute_reply":"2023-03-28T08:17:26.030452Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f\"r2 score after removing 'gross_weight', 'low_fat', 'recyclable_package', 'units_per_case': {r2_score(y_all_rms, _oof_preds_all_rms)}\")\nprint(f\"validation rmslse after removing 'gross_weight', 'low_fat', 'recyclable_package', 'units_per_case': \",mean_valid_score_all_rms)","metadata":{"execution":{"iopub.status.busy":"2023-03-28T08:17:26.033284Z","iopub.execute_input":"2023-03-28T08:17:26.033610Z","iopub.status.idle":"2023-03-28T08:17:26.045263Z","shell.execute_reply.started":"2023-03-28T08:17:26.033582Z","shell.execute_reply":"2023-03-28T08:17:26.044110Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_temp = test.copy()\ntest_temp['extra_attraction'] = test_temp['florist'] + test_temp['video_store'] + test_temp['prepared_food'] + test_temp['coffee_bar']\ntest_temp['children'] = test_temp['total_children'] * test_temp['num_children_at_home']\ntest_temp['children*avg_cars_at home(approx).1'] = test_temp['children'] * test_temp['avg_cars_at home(approx).1']\n\n\ntest_preds_all_rms = predict_test(_models_all_rms, test_temp[features_all_rms], gbdt=True, n_splits=10, n_repeats=3)","metadata":{"execution":{"iopub.status.busy":"2023-03-28T08:17:26.047064Z","iopub.execute_input":"2023-03-28T08:17:26.047427Z","iopub.status.idle":"2023-03-28T08:17:39.417975Z","shell.execute_reply.started":"2023-03-28T08:17:26.047390Z","shell.execute_reply":"2023-03-28T08:17:39.416864Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_all_rms = submission_csv(test_preds_all_rms)\ndf_all_rms.to_csv(\"xgb_all_rms_improved_correct.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2023-03-28T08:17:39.419321Z","iopub.execute_input":"2023-03-28T08:17:39.419695Z","iopub.status.idle":"2023-03-28T08:17:39.899604Z","shell.execute_reply.started":"2023-03-28T08:17:39.419657Z","shell.execute_reply":"2023-03-28T08:17:39.898571Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The score has increased. But store_sqft is very important feature but the model was not able to capture it. Let ","metadata":{}},{"cell_type":"markdown","source":"****SHAP(SHapley Additive exPlanations)****\n\nLearn about shap from [here](https://www.kaggle.com/code/dansbecker/shap-values) ","metadata":{}},{"cell_type":"code","source":"X_shap = train.drop(columns=['id', 'cost', 'salad_bar', 'gross_weight', 'low_fat', \n                            'recyclable_package', 'units_per_case', 'store_sales(in millions)', 'unit_sales(in millions)'])\ny_shap = np.log(train['cost'])\nfeatures_par = X_shap.columns.tolist()\nadd_data = data.loc[data['generated'] == False]\nadd_data = add_data[features_par+ ['cost']]\n\nX_shap['extra_attraction'] = X_shap['florist'] + X_shap['video_store'] + X_shap['prepared_food'] + X_shap['coffee_bar']\nX_shap['children'] = X_shap['total_children'] * X_shap['num_children_at_home']\nX_shap['children*avg_cars_at home(approx).1'] = X_shap['children'] * X_shap['avg_cars_at home(approx).1']\n\nadd_data['extra_attraction'] = add_data['florist'] + add_data['video_store'] + add_data['prepared_food'] + add_data['coffee_bar']\nadd_data['children'] = add_data['total_children'] * add_data['num_children_at_home']\nadd_data['children*avg_cars_at home(approx).1'] = add_data['children'] * add_data['avg_cars_at home(approx).1']\n\n\n\nsplitter = Splitter(kfold=False)\nX_train, X_val, y_train, y_val = next(splitter.split_data(X_shap, y_shap, random_state=5))\nX_train = pd.concat([X_train, add_data.drop('cost', axis=1)])\ny_train = pd.concat([y_train, np.log(add_data['cost'])])\n\n_model_shap = xgb()\n_model_shap.fit(X_train, y_train)","metadata":{"execution":{"iopub.status.busy":"2023-03-28T08:17:39.901308Z","iopub.execute_input":"2023-03-28T08:17:39.901711Z","iopub.status.idle":"2023-03-28T08:17:40.633755Z","shell.execute_reply.started":"2023-03-28T08:17:39.901673Z","shell.execute_reply":"2023-03-28T08:17:40.632955Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import shap\n\nexplainer = shap.TreeExplainer(_model_shap)\n\n# Calculate Shap values\nshap_values = explainer.shap_values(X_val)","metadata":{"execution":{"iopub.status.busy":"2023-03-28T08:17:40.637132Z","iopub.execute_input":"2023-03-28T08:17:40.637422Z","iopub.status.idle":"2023-03-28T08:17:44.598338Z","shell.execute_reply.started":"2023-03-28T08:17:40.637395Z","shell.execute_reply":"2023-03-28T08:17:44.597214Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"i = 7\nshap.initjs()\nshap.force_plot(explainer.expected_value, shap_values[i], X_val.iloc[i] ,feature_names=X_val.columns.to_list())","metadata":{"execution":{"iopub.status.busy":"2023-03-28T08:17:44.599724Z","iopub.execute_input":"2023-03-28T08:17:44.600104Z","iopub.status.idle":"2023-03-28T08:17:44.621727Z","shell.execute_reply.started":"2023-03-28T08:17:44.600060Z","shell.execute_reply":"2023-03-28T08:17:44.620816Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"i = 5\nshap.initjs()\nshap.force_plot(explainer.expected_value, shap_values[i], X_val.iloc[i] ,feature_names=X_val.columns.to_list())","metadata":{"execution":{"iopub.status.busy":"2023-03-28T08:17:44.623243Z","iopub.execute_input":"2023-03-28T08:17:44.629235Z","iopub.status.idle":"2023-03-28T08:17:44.647100Z","shell.execute_reply.started":"2023-03-28T08:17:44.629200Z","shell.execute_reply":"2023-03-28T08:17:44.646236Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# shap.force_plot(explainer.expected_value, shap_values[0:6,:], X_val.iloc[0:6,:], plot_cmap=\"DrDb\",\n#                 feature_names=X_val.columns.to_list())","metadata":{"execution":{"iopub.status.busy":"2023-03-28T08:17:44.648452Z","iopub.execute_input":"2023-03-28T08:17:44.649031Z","iopub.status.idle":"2023-03-28T08:17:44.654404Z","shell.execute_reply.started":"2023-03-28T08:17:44.648988Z","shell.execute_reply":"2023-03-28T08:17:44.653249Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"shap.summary_plot(shap_values, X_val, feature_names=X_val.columns.tolist())","metadata":{"execution":{"iopub.status.busy":"2023-03-28T08:17:44.655995Z","iopub.execute_input":"2023-03-28T08:17:44.657212Z","iopub.status.idle":"2023-03-28T08:17:51.152043Z","shell.execute_reply.started":"2023-03-28T08:17:44.657174Z","shell.execute_reply":"2023-03-28T08:17:51.150851Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"shap.dependence_plot('store_sqft', shap_values, X_val, interaction_index='avg_cars_at home(approx).1')","metadata":{"execution":{"iopub.status.busy":"2023-03-28T08:17:51.153618Z","iopub.execute_input":"2023-03-28T08:17:51.154307Z","iopub.status.idle":"2023-03-28T08:17:51.769877Z","shell.execute_reply.started":"2023-03-28T08:17:51.154263Z","shell.execute_reply":"2023-03-28T08:17:51.768748Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"shap.dependence_plot('store_sqft', shap_values, X_val)","metadata":{"execution":{"iopub.status.busy":"2023-03-28T08:17:51.771449Z","iopub.execute_input":"2023-03-28T08:17:51.771933Z","iopub.status.idle":"2023-03-28T08:17:52.818826Z","shell.execute_reply.started":"2023-03-28T08:17:51.771894Z","shell.execute_reply":"2023-03-28T08:17:52.817764Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# sns.scatterplot(data=train.iloc[:1000], y='cost', x='id', hue='store_sqft', size='store_sqft')","metadata":{"execution":{"iopub.status.busy":"2023-03-28T08:17:52.820621Z","iopub.execute_input":"2023-03-28T08:17:52.821374Z","iopub.status.idle":"2023-03-28T08:17:52.826017Z","shell.execute_reply.started":"2023-03-28T08:17:52.821322Z","shell.execute_reply":"2023-03-28T08:17:52.824859Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2023-03-28T08:17:52.827368Z","iopub.execute_input":"2023-03-28T08:17:52.827891Z","iopub.status.idle":"2023-03-28T08:17:53.206397Z","shell.execute_reply.started":"2023-03-28T08:17:52.827854Z","shell.execute_reply":"2023-03-28T08:17:53.205256Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"need to change evalue_model and predict model a little","metadata":{}},{"cell_type":"code","source":"# need to change evaluation method a little\ndef evaluate_model(model_name, model, _X, _y, gbdt=True, original_data=None, use_original=False, n_splits=5, random_state_list=[0, 5, 10]):\n    len_y = len(_y)\n    len_states = len(random_state_list)\n\n    oof_preds = np.zeros(len_y * len_states).reshape(len_states, len_y)\n    models = []\n    scores_train = []\n    scalers = []\n    encoders = []\n    for index, random_state in enumerate(random_state_list):\n        print(\"#\"*25)\n        print(\"#\"*15, f\"traininng model {model_name} with seed {random_state}\")\n        print(\"#\"*25)\n        splitter = Splitter(n_splits=n_splits)\n        splits = 0\n        for X_train, X_val, y_train, y_val, train_idx, val_idx in splitter.split_data(_X, _y, random_state):\n    \n            \n            if use_original: # we will only use original data for training not testing\n                target = 'cost'\n                X_train = pd.concat([X_train, original_data.drop(target, axis=1)]) \n                y_train = pd.concat([y_train, np.log(original_data[target])]) # only for \n                \n#             all preprocessing will happen after this step\n\n            if not gbdt:  \n                scaler = StandardScaler()\n                X_train = scaler.fit_transform(X_train)\n                X_val = scaler.transform(X_val)\n                scalers.append(deepcopy(scaler))\n              \n            \n            encoder = MEstimateEncoder(cols='store_sqft_encode', m=10)\n#             encoder = TargetEncoder(cols='store_sqft_encode')\n            X_train = encoder.fit_transform(X_train, y_train)\n            X_val = encoder.transform(X_val)\n            encoders.append(deepcopy(encoder))\n\n\n            _model = model()\n            _model.fit(X_train, y_train)\n            oof_preds[index, val_idx] = _model.predict(X_val).squeeze()\n            models.append(deepcopy(_model))\n\n            score_train = root_mean_squared_log_error(y_train, _model.predict(X_train))\n            scores_train.append(score_train)\n\n            score_valid_split = root_mean_squared_log_error(y_val, _model.predict(X_val).squeeze())\n\n            print(f\"seed {random_state} and split {splits} score {score_valid_split}\")\n            splits += 1\n            \n\n        \n        X_train.head()\n\n    oof_preds_mean = oof_preds.mean(axis=0)\n\n    return models, oof_preds_mean, np.mean(scores_train), root_mean_squared_log_error(_y, oof_preds_mean), scalers, encoders\n\n\ndef predict_test(models, X_test, scalers=None, encoders=None, gbdt=True, n_splits=5, n_repeats=3):\n    test_preds = np.zeros(n_splits * n_repeats * len(X_test)).reshape(n_splits * n_repeats, len(X_test))\n    for index, model in enumerate(models):\n        X_test_ = X_test.copy()\n        if not gbdt:\n            X_test_ = scalers[index].transform(X_test_) # normally you would not do ensembling for linear models \n        \n        if encoders:\n#             print(\"Sdjf\")\n            X_test_ = encoders[index].transform(X_test_)\n#             display(X_test_.head(3))\n        preds = model.predict(X_test_)                 # if we have a lot of observations we can do that if we want\n        test_preds[index, range(len(preds))] = preds\n        \n    return test_preds.mean(axis=0)\n","metadata":{"execution":{"iopub.status.busy":"2023-03-28T08:17:53.208697Z","iopub.execute_input":"2023-03-28T08:17:53.209364Z","iopub.status.idle":"2023-03-28T08:17:53.226121Z","shell.execute_reply.started":"2023-03-28T08:17:53.209323Z","shell.execute_reply":"2023-03-28T08:17:53.225095Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# success\nX_all_rms = train.drop(columns=['id', 'cost', 'salad_bar', 'gross_weight', 'low_fat', 'recyclable_package', 'units_per_case', 'store_sales(in millions)', 'unit_sales(in millions)'])\ny_all_rms = np.log(train['cost'])\nfeatures_all_rms = X_all_rms.columns.tolist()\nadd_data = data.loc[data['generated'] == False]\nadd_data = add_data[features_all_rms + ['cost']]\n\n# creating features\nX_all_rms['extra_attraction'] = X_all_rms['florist'] + X_all_rms['video_store'] + X_all_rms['prepared_food'] + X_all_rms['coffee_bar']\nX_all_rms['children'] = X_all_rms['total_children'] * X_all_rms['num_children_at_home']\nX_all_rms['children*avg_cars_at home(approx).1'] = X_all_rms['total_children'] * X_all_rms['num_children_at_home'] * X_all_rms['avg_cars_at home(approx).1']\nX_all_rms['store_sqft_encode'] = X_all_rms['store_sqft'].copy()\n\nadd_data['extra_attraction'] = add_data['florist'] + add_data['video_store'] + add_data['prepared_food'] + add_data['coffee_bar']\nadd_data['children'] = add_data['total_children'] * add_data['num_children_at_home']\nadd_data['children*avg_cars_at home(approx).1'] = add_data['total_children'] * add_data['num_children_at_home']* add_data['avg_cars_at home(approx).1']\nadd_data['store_sqft_encode'] = add_data['store_sqft'].copy()\nfeatures_all_rms = X_all_rms.columns.tolist()\n\n_models_all_rms, _oof_preds_all_rms, mean_train_score_all_rms, mean_valid_score_all_rms, _scalers_all_rms, encoders_all_rms = evaluate_model(\"XGBRegressor_default\", xgb, \n                                                                X_all_rms, y_all_rms , use_original=True, original_data=add_data, gbdt=True, n_splits=10, random_state_list=[0, 5, 10]) ","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2023-03-28T08:17:53.228760Z","iopub.execute_input":"2023-03-28T08:17:53.229858Z","iopub.status.idle":"2023-03-28T08:19:02.414319Z","shell.execute_reply.started":"2023-03-28T08:17:53.229820Z","shell.execute_reply":"2023-03-28T08:19:02.413433Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f\"Best score till now: \", mean_valid_score_all_rms)","metadata":{"execution":{"iopub.status.busy":"2023-03-28T08:19:02.416311Z","iopub.execute_input":"2023-03-28T08:19:02.417378Z","iopub.status.idle":"2023-03-28T08:19:02.423750Z","shell.execute_reply.started":"2023-03-28T08:19:02.417326Z","shell.execute_reply":"2023-03-28T08:19:02.422601Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_temp = test.copy()\ntest_temp['extra_attraction'] = test_temp['florist'] + test_temp['video_store'] + test_temp['prepared_food'] + test_temp['coffee_bar']\ntest_temp['children'] = test_temp['total_children'] * test_temp['num_children_at_home']\ntest_temp['children*avg_cars_at home(approx).1'] = test_temp['children'] * test_temp['avg_cars_at home(approx).1']\ntest_temp['store_sqft_encode'] = test_temp['store_sqft'].copy()\n\ntest_preds_all_rms = predict_test(_models_all_rms, test_temp[features_all_rms], encoders=encoders_all_rms, gbdt=True, n_splits=10, n_repeats=3)","metadata":{"execution":{"iopub.status.busy":"2023-03-28T08:19:02.425495Z","iopub.execute_input":"2023-03-28T08:19:02.425867Z","iopub.status.idle":"2023-03-28T08:19:19.417778Z","shell.execute_reply.started":"2023-03-28T08:19:02.425831Z","shell.execute_reply":"2023-03-28T08:19:19.416581Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_encode_store_sqft = submission_csv(test_preds_all_rms)","metadata":{"execution":{"iopub.status.busy":"2023-03-28T08:19:19.419450Z","iopub.execute_input":"2023-03-28T08:19:19.419856Z","iopub.status.idle":"2023-03-28T08:19:19.443452Z","shell.execute_reply.started":"2023-03-28T08:19:19.419812Z","shell.execute_reply":"2023-03-28T08:19:19.441968Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_encode_store_sqft.to_csv(\"xgb_all_rms_encode.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2023-03-28T08:19:19.445447Z","iopub.execute_input":"2023-03-28T08:19:19.445929Z","iopub.status.idle":"2023-03-28T08:19:19.927880Z","shell.execute_reply.started":"2023-03-28T08:19:19.445865Z","shell.execute_reply":"2023-03-28T08:19:19.926768Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Lest combine some other models and build an ensemble model","metadata":{}},{"cell_type":"code","source":"# from lightgbm import LGBMRegressor\n# from catboost import CatBoostRegressor\n# xgb = partial(XGBRegressor, tree_method='gpu_hist', random_state=0)\n# cat = partial(CatBoostRegressor, random_state=0, verbose=False, task_type=\"GPU\")\n# lgbm = partial(LGBMRegressor, random_state=0, device='gpu')","metadata":{"execution":{"iopub.status.busy":"2023-03-28T08:19:19.929285Z","iopub.execute_input":"2023-03-28T08:19:19.930375Z","iopub.status.idle":"2023-03-28T08:19:19.936071Z","shell.execute_reply.started":"2023-03-28T08:19:19.930336Z","shell.execute_reply":"2023-03-28T08:19:19.935028Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # success\n# X_all_rms = train.drop(columns=['id', 'cost', 'salad_bar', 'gross_weight', 'low_fat', 'recyclable_package', 'units_per_case', 'store_sales(in millions)', 'unit_sales(in millions)'])\n# y_all_rms = np.log(train['cost'])\n# features_all_rms = X_all_rms.columns.tolist()\n# add_data = data.loc[data['generated'] == False]\n# add_data = add_data[features_all_rms + ['cost']]\n\n# # creating features\n# X_all_rms['extra_attraction'] = X_all_rms['florist'] + X_all_rms['video_store'] + X_all_rms['prepared_food'] + X_all_rms['coffee_bar']\n# X_all_rms['children'] = X_all_rms['total_children'] * X_all_rms['num_children_at_home']\n# X_all_rms['children*avg_cars_at home(approx).1'] = X_all_rms['total_children'] * X_all_rms['num_children_at_home'] * X_all_rms['avg_cars_at home(approx).1']\n# X_all_rms['store_sqft_encode'] = X_all_rms['store_sqft'].copy()\n\n# add_data['extra_attraction'] = add_data['florist'] + add_data['video_store'] + add_data['prepared_food'] + add_data['coffee_bar']\n# add_data['children'] = add_data['total_children'] * add_data['num_children_at_home']\n# add_data['children*avg_cars_at home(approx).1'] = add_data['total_children'] * add_data['num_children_at_home']* add_data['avg_cars_at home(approx).1']\n# add_data['store_sqft_encode'] = add_data['store_sqft'].copy()\n# features_all_rms = X_all_rms.columns.tolist()\n\n# # _models_all_rms_xgb, _oof_preds_all_rms_xgb, mean_train_score_all_rms_xgb, mean_valid_score_all_rms_xgb, _scalers_all_rms_xgb, encoders_all_rms_xgb = evaluate_model(\"XGBRegressor_default\", xgb, \n# #                                                                 X_all_rms, y_all_rms , use_original=True, original_data=add_data, gbdt=True, n_splits=10, random_state_list=[0, 5, 10]) \n\n# _models_all_rms_cat, _oof_preds_all_rms_cat, mean_train_score_all_rms_cat, mean_valid_score_all_rms_cat, _scalers_all_rms_cat, encoders_all_rms_cat = evaluate_model(\"catboost_default\", cat, \n#                                                                 X_all_rms, y_all_rms , use_original=True, original_data=add_data, gbdt=True, n_splits=10, random_state_list=[0, 5, 10]) \n","metadata":{"execution":{"iopub.status.busy":"2023-03-28T08:19:19.937530Z","iopub.execute_input":"2023-03-28T08:19:19.938118Z","iopub.status.idle":"2023-03-28T08:19:19.946978Z","shell.execute_reply.started":"2023-03-28T08:19:19.938076Z","shell.execute_reply":"2023-03-28T08:19:19.946022Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# mean_valid_score_all_rms_cat","metadata":{"execution":{"iopub.status.busy":"2023-03-28T08:19:19.949977Z","iopub.execute_input":"2023-03-28T08:19:19.950374Z","iopub.status.idle":"2023-03-28T08:19:19.961432Z","shell.execute_reply.started":"2023-03-28T08:19:19.950335Z","shell.execute_reply":"2023-03-28T08:19:19.960279Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# root_mean_squared_log_error(y_all_rms, (_oof_preds_all_rms_lgbm + _oof_preds_all_rms)/2)","metadata":{"execution":{"iopub.status.busy":"2023-03-28T08:19:19.962776Z","iopub.execute_input":"2023-03-28T08:19:19.963654Z","iopub.status.idle":"2023-03-28T08:19:19.970519Z","shell.execute_reply.started":"2023-03-28T08:19:19.963612Z","shell.execute_reply":"2023-03-28T08:19:19.969548Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # success\n# X_all_rms = train.drop(columns=['id', 'cost', 'salad_bar', 'gross_weight', 'low_fat', 'recyclable_package', 'units_per_case', 'store_sales(in millions)', 'unit_sales(in millions)'])\n# y_all_rms = np.log(train['cost'])\n# features_all_rms = X_all_rms.columns.tolist()\n# add_data = data.loc[data['generated'] == False]\n# add_data = add_data[features_all_rms + ['cost']]\n\n# # creating features\n# X_all_rms['extra_attraction'] = X_all_rms['florist'] + X_all_rms['video_store'] + X_all_rms['prepared_food'] + X_all_rms['coffee_bar']\n# X_all_rms['children'] = X_all_rms['total_children'] * X_all_rms['num_children_at_home']\n# X_all_rms['children*avg_cars_at home(approx).1'] = X_all_rms['total_children'] * X_all_rms['num_children_at_home'] * X_all_rms['avg_cars_at home(approx).1']\n# X_all_rms['store_sqft_encode'] = X_all_rms['store_sqft'].copy()\n\n# add_data['extra_attraction'] = add_data['florist'] + add_data['video_store'] + add_data['prepared_food'] + add_data['coffee_bar']\n# add_data['children'] = add_data['total_children'] * add_data['num_children_at_home']\n# add_data['children*avg_cars_at home(approx).1'] = add_data['total_children'] * add_data['num_children_at_home']* add_data['avg_cars_at home(approx).1']\n# add_data['store_sqft_encode'] = add_data['store_sqft'].copy()\n# features_all_rms = X_all_rms.columns.tolist()\n\n# _models_all_rms, _oof_preds_all_rms, mean_train_score_all_rms, mean_valid_score_all_rms, _scalers_all_rms, encoders_all_rms = evaluate_model(\"XGBRegressor_default\", xgb, \n#                                                                 X_all_rms, y_all_rms , use_original=True, original_data=add_data, gbdt=True, n_splits=10, random_state_list=[0, 5, 10]) ","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2023-03-28T08:19:19.971918Z","iopub.execute_input":"2023-03-28T08:19:19.972368Z","iopub.status.idle":"2023-03-28T08:19:19.981299Z","shell.execute_reply.started":"2023-03-28T08:19:19.972319Z","shell.execute_reply":"2023-03-28T08:19:19.980320Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # success\n# X_all_rms = train.drop(columns=['id', 'cost', 'salad_bar', 'gross_weight', 'low_fat', 'recyclable_package', 'units_per_case', 'store_sales(in millions)', 'unit_sales(in millions)'])\n# y_all_rms = np.log(train['cost'])\n# features_all_rms = X_all_rms.columns.tolist()\n# add_data = data.loc[data['generated'] == False]\n# add_data = add_data[features_all_rms + ['cost']]\n\n# # creating features\n# X_all_rms['extra_attraction'] = X_all_rms['florist'] + X_all_rms['video_store'] + X_all_rms['prepared_food'] + X_all_rms['coffee_bar']\n# X_all_rms['children'] = X_all_rms['total_children'] * X_all_rms['num_children_at_home']\n# X_all_rms['children*avg_cars_at home(approx).1'] = X_all_rms['total_children'] * X_all_rms['num_children_at_home'] * X_all_rms['avg_cars_at home(approx).1']\n# X_all_rms['store_sqft_encode'] = X_all_rms['store_sqft'].copy()\n# X_all_rms['children/avg_cars_at home(approx).1'] = (X_all_rms['children'] / X_all_rms['avg_cars_at home(approx).1']).clip(None, 100)\n\n# add_data['extra_attraction'] = add_data['florist'] + add_data['video_store'] + add_data['prepared_food'] + add_data['coffee_bar']\n# add_data['children'] = add_data['total_children'] * add_data['num_children_at_home']\n# add_data['children*avg_cars_at home(approx).1'] = add_data['total_children'] * add_data['num_children_at_home']* add_data['avg_cars_at home(approx).1']\n# add_data['store_sqft_encode'] = add_data['store_sqft'].copy()\n# add_data['children/avg_cars_at home(approx).1'] = (add_data['children'] / add_data['avg_cars_at home(approx).1']).clip(None, 100)\n# features_all_rms = X_all_rms.columns.tolist()\n\n# _models_all_rms, _oof_preds_all_rms, mean_train_score_all_rms, mean_valid_score_all_rms, _scalers_all_rms = evaluate_model(\"XGBRegressor_default\", xgb, \n#                                                                 X_all_rms, y_all_rms , use_original=True, original_data=add_data, gbdt=True, n_splits=10, random_state_list=[0, 5, 10]) ","metadata":{"execution":{"iopub.status.busy":"2023-03-28T08:19:19.993175Z","iopub.execute_input":"2023-03-28T08:19:19.993450Z","iopub.status.idle":"2023-03-28T08:19:19.999415Z","shell.execute_reply.started":"2023-03-28T08:19:19.993423Z","shell.execute_reply":"2023-03-28T08:19:19.998354Z"},"trusted":true},"execution_count":null,"outputs":[]}]}